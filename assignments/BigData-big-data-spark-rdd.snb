{
  "metadata" : {
    "name" : "BigData-big-data-spark-rdd",
    "user_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : {
      "spark.app.name" : "Notebook",
      "spark.master" : "local[8]",
      "spark.executor.memory" : "1G"
    }
  },
  "cells" : [ {
    "metadata" : {
      "id" : "FBE76734AF7B4C71875E497B740C990E"
    },
    "cell_type" : "markdown",
    "source" : "# Big Data: Spark RDD\n\n## Getting acquainted with Spark and Spark Notebook\n\n Never used a Notebook? \n Find useful advice in the UI tour (in the help menu) or in the \n [Spark Notebook documentation](https://github.com/spark-notebook/spark-notebook/blob/master/docs/exploring_notebook.md) itself.\n \n## First steps Spark and Scala\n\n### My First RDD\n\nRDDs can be initiated from in-memory collections or from files in the (distributed or local) file system.\n\nLet's first initialize an RDD from a collection. The second parameter is optional, and instructs the platform to split the data in 8 partitions."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "DA4E277FAF7C4B778E8ABB13B4B9AFB8"
    },
    "cell_type" : "code",
    "source" : "val rdd = sc.parallelize(0 to 999,8)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "4BFD4BBEBE904EF68E25F4C7EA3BCA39"
    },
    "cell_type" : "markdown",
    "source" : "Remember that evaluation is lazy, and only happens upon actions, not transformations; i.e., so far, nothing happened.\n\n_Check:_ Spark UI: [stages](http://localhost:4040/stages/) is still empty."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab18227827-0\"\n}"
      },
      "id" : "4BE5337693CC44BF82E9ABCB48106FF4"
    },
    "cell_type" : "code",
    "source" : "val sample = rdd.takeSample(false, 4)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "545599137D8E4CEB8844C430F48BA0CC"
    },
    "cell_type" : "markdown",
    "source" : "Only now, evaluation took place: see the [stages](http://localhost:4040/stages/) in the Spark UI.\nClick on the links!\n_Why are there two jobs?_"
  }, {
    "metadata" : {
      "id" : "24BABECC852D44758D23663C539DD7ED"
    },
    "cell_type" : "markdown",
    "source" : "### Data\n\nUse a shell escape to test if the Gutenberg data was correctly loaded on the docker container running the Spark Notebook.\n\nNote: [Assignment 2](http://rubigdata.github.io/course/assignments/A2-mapreduce.html) gives instructions how to get the full Shakespeare on your Spark Notebook container."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "3F206A4C70FF4C9680631DF621FB0CCE"
    },
    "cell_type" : "code",
    "source" : ":sh ls /mnt/bigdata",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "A8195BB9B8624DFC80DADD10D7E6881E"
    },
    "cell_type" : "markdown",
    "source" : "### Counting words\n\nWe will use the Shakespeare data for the classic Big Data \"Hello World\" exercise, counting words."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "2338CDA0C3F743EE83C5FC5342E533F4"
    },
    "cell_type" : "code",
    "source" : "val lines = sc.textFile(\"/mnt/bigdata/100.txt\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "B057B5D2E84548BCB735D89DD152C141"
    },
    "cell_type" : "markdown",
    "source" : "Can you predict what the following commands will do?\nRecognize the Map Reduce pattern on lines 2 and 3?"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "23D1AF9D2CE1411E8B6F999DC05F63D3"
    },
    "cell_type" : "code",
    "source" : "println( \"Lines:\\t\", lines.count, \"\\n\" + \n         \"Chars:\\t\", lines.map(s => s.length).\n                           reduce((a, b) => a + b))",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "00E7284A28DF42E7898D5F04431534D6"
    },
    "cell_type" : "markdown",
    "source" : "The map operator executes its parameter, the lambda function, on every item in the RDD.\nReduce is also defined using a lambda function.\n\n_Note:_ if you never took a functional programming course, look at [this answer on StackExchange](http://stackoverflow.com/a/16509/2127435).\n\nNow try to understand in detail the following example.\nWhy do we use `flatMap` and not `map`?\n\nIt is worth copying the cell, and inspecting output at intermediate steps (use `take()`, not `collect()` - _do you remember why?_)."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "0718A7BDEFFA4E5B8A5F6B1DB6BCBDE4"
    },
    "cell_type" : "code",
    "source" : "val words = lines.flatMap(line => line.split(\" \"))\n              .filter(_ != \"\")\n              .map(word => (word,1))",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "C10358C3D581449683E5E3EA737012F3"
    },
    "cell_type" : "code",
    "source" : "val wc = words.reduceByKey(_ + _)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab2014657358-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "397206C23B6748F68657C4441680CB72"
    },
    "cell_type" : "code",
    "source" : "wc.take(10)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "A77042B6782C45D395B30CA193D344EB"
    },
    "cell_type" : "markdown",
    "source" : "Take a look at how the platform processes this query:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "1B893A39F6724E20BEE3715A68BC813E"
    },
    "cell_type" : "code",
    "source" : "wc.toDebugString",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "F431CE5F87464C1A82606D456A883CE4"
    },
    "cell_type" : "markdown",
    "source" : "Inspect the Spark UI to see the computations in the cluster â†’ \n[see stages](http://localhost:4040/stages/) and their constituent tasks."
  }, {
    "metadata" : {
      "id" : "7B8904D8F4464B5D9EF451DE82482F98"
    },
    "cell_type" : "markdown",
    "source" : "### To count or not to count\nOk, we can count words - let us find out which words Shakespeare used most often!"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab758044651-0\"\n}"
      },
      "id" : "50F56A1566844BFB81CD15193D77C590"
    },
    "cell_type" : "code",
    "source" : "val top10 = wc.takeOrdered(10)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "00F186A4DB7F410C93A01D8D4E6BD7C2"
    },
    "cell_type" : "markdown",
    "source" : "Ok, not quite what we wanted!\nSee what's wrong?\n\nLet's fix the result ordering as follows."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab1387326668-0\"\n}"
      },
      "id" : "C32EE3BE19744CAF8752B47834966A9B"
    },
    "cell_type" : "code",
    "source" : "val top10 = wc.takeOrdered(10)(Ordering[Int].reverse.on(x=>x._2))",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "3CEFFF2741F045CC8326532D7C30239F"
    },
    "cell_type" : "markdown",
    "source" : "You can render the collected results however you want to using the client programming language."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab1441758645-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "80879A64B623457183DBD741D1A4A034"
    },
    "cell_type" : "code",
    "source" : "top10.map({case(w,c) => \"Word '%s' occurs %d times\".format(w,c)}).map(println)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "71C7F0526A97490788D2F6BF15C50026"
    },
    "cell_type" : "markdown",
    "source" : "We can zoom in on specific word frequencies, that might be more interesting than stopwords!"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab1384716844-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "022554E0682B49429818282660C07E98"
    },
    "cell_type" : "code",
    "source" : "wc.filter(_._1 == \"Romeo\").collect",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab609530909-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "412B8D1CA6D24D7E86B4FC001D530F56"
    },
    "cell_type" : "code",
    "source" : "wc.filter(_._1 == \"Julia\").collect",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "31E04AFEBE4B40B2A8D36536E7D3E491"
    },
    "cell_type" : "code",
    "source" : "wc.cache()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab665811935-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "82169E8C023C4E919C383CD2F6A6901D"
    },
    "cell_type" : "code",
    "source" : "wc.filter(_._1 == \"Macbeth\").collect",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab84274309-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "E37D99D4857F457B802D4A49E4CA8582"
    },
    "cell_type" : "code",
    "source" : "wc.filter(_._1 == \"Capulet\").collect",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "A9AA7568D5EE45D785975A1131085309"
    },
    "cell_type" : "markdown",
    "source" : "Many different ways exist to compute the top N results. A few follow - _try to understand what actual work (for the cluster) is actually generated by the various alternatives._"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "AAD7932FEAD345178AF23D96445985B3"
    },
    "cell_type" : "code",
    "source" : "val oCounts = wc.map(x => x._2 -> x._1).sortByKey(false).map(x => x._2 -> x._1).cache()\noCounts.take(10).foreach(println)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "3F8A955F7B064805886B1FCC255E7081"
    },
    "cell_type" : "code",
    "source" : "// Alternative way to achieve the same:\nwc.sortBy(x => -x._2).take(10).foreach(println)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "465F5AADC3D647FC9030D3302BA05329"
    },
    "cell_type" : "code",
    "source" : "// Preferred way if you really just want the top results\n// Note that you do not first need to assign the ordering function to a variable - you could just pass along the Ordering.by expression instead.\nval asc = (Ordering.by[(String, Int), Int](_._2))\nwc.top(10)(asc).foreach(println)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "50047376F71F48D098CCE2961195D5EE"
    },
    "cell_type" : "code",
    "source" : "// Alternative formulation\nval desc = (Ordering.by[(String, Int), Int](-_._2))\nwc.takeOrdered(10)(desc).foreach(println)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "F8E29A81973A428586DAF1530FBCDDA7"
    },
    "cell_type" : "markdown",
    "source" : "The next section saves the results of word counting in the filesystem. \n\nWe use a simple shell command to look into the directory that has been created.\n(Alternatively, you can navigate the filesystem after issuing a `docker exec -it HASH bash` command on the machine running the notebook container.)"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "85441E57359B42C2839BCCAF76482CBC"
    },
    "cell_type" : "code",
    "source" : "words.saveAsTextFile(\"wc\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "A24D35BEA86A4F5B8566BFC83FC2449C"
    },
    "cell_type" : "code",
    "source" : ":sh ls wc",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "F87FDE9370F44F0E86781659B54ABFC9"
    },
    "cell_type" : "markdown",
    "source" : "_Q: Explain why there are multiple result files._\n\nInspect the files from the command line in the docker container (using `docker exec`)."
  }, {
    "metadata" : {
      "id" : "A5743005ED4344CC8ECA842A0EB81403"
    },
    "cell_type" : "markdown",
    "source" : "Clean up the directory to save headaches when later rerunning the notebook."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "EFE613DC0ED14B5999023336084A7123"
    },
    "cell_type" : "code",
    "source" : ":sh rm -rf wc",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "1FF056C361C74E14900145A8AA82A39E"
    },
    "cell_type" : "markdown",
    "source" : "### How to count?"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "940B8DC6670747C4B36B2EC22A1BA470"
    },
    "cell_type" : "code",
    "source" : "val words = lines.flatMap(line => line.split(\" \"))\n              .map(w => w.toLowerCase().replaceAll(\"(^[^a-z]+|[^a-z]+$)\", \"\"))\n              .filter(_ != \"\")\n              .map(w => (w,1))\n              .reduceByKey( _ + _ )",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab1918255847-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "278814F8FBE749A7846C6BD58BA18FB9"
    },
    "cell_type" : "code",
    "source" : "words.filter(_._1 == \"macbeth\").collect\n  .map({case (w,c) => \"%s occurs %d times\".format(w,c)}).map(println)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "89BBC292248E453C880A6C12DE59693C"
    },
    "cell_type" : "markdown",
    "source" : "_Q: why are the counts different?_"
  }, {
    "metadata" : {
      "id" : "C3243A44228E43A5858434D90500EE52"
    },
    "cell_type" : "markdown",
    "source" : "## Deeper Understanding of Spark\n\nThe goal of the next steps in the assignment is to gain a deeper understanding in what happens \"inside\".\n\nDo not just click shift enter on every cell, but try to grasp what is happening by looking into the Spark UI after executing each command.\n\nYou should try to learn by using variants of the example queries; create a new cell, or duplicate the cell and modify it."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "792C2F486B5C4A2EB83A4F80897C44B3"
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.HashPartitioner",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "F6F7338FA12446948DEA1A201C96E970"
    },
    "cell_type" : "code",
    "source" : "val rdd1 = sc.parallelize(0 to 999,8)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "A4167B271EDD444C894A6EB69C0B0557"
    },
    "cell_type" : "markdown",
    "source" : "Remember that evaluation is lazy, and only happens upon actions, not transformations; i.e., so far, nothing happened. \n\nCheck the Spark UI: jobs and stages do not yet contain entries corresponding to this command.\n\n_Note: Port number :4040 can be higher, e.g., :4041, depending on how many notebooks you have running in the kernel._"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "124B1DBAB9DE4EF5B89129165299106A"
    },
    "cell_type" : "code",
    "source" : "printf( \"Number of partitions: %d\\n\", rdd1.partitions.length)\nrdd1.partitioner",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "00EC4570F76F46969AF856977E10A1B9"
    },
    "cell_type" : "markdown",
    "source" : "The default number of partitions depends on the number of cores in the machine that runs the docker engine; see e.g., `cat /proc/cpuinfo` from a `bash` shell in the docker engine."
  }, {
    "metadata" : {
      "id" : "85CE74955E7442E581F0D0ED58C9B604"
    },
    "cell_type" : "markdown",
    "source" : "The next command creates pairs of numbers, that we will treat as key-value pairs in the remainder of this notebook.\n\nIf you have not looked at Spark documentation so far, this would be a good time to go through the information on transformations and PairRDD functions:\n- https://spark.apache.org/docs/latest/programming-guide.html#transformations\n- https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "D0B0F0E7391445A8AB17B2F9081D9E74"
    },
    "cell_type" : "code",
    "source" : "val rddp = rdd1.map(x => (x % 100, 1000 - x))",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab1210493294-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "2C16B0B59808456F8953FEE71BBEB603"
    },
    "cell_type" : "code",
    "source" : "rddp.take(15)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "4850FBD47B3B444E8BCCB1607B269A56"
    },
    "cell_type" : "code",
    "source" : "rddp.partitioner",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "78584CADC11A4091AC360A52DD74C760"
    },
    "cell_type" : "markdown",
    "source" : "As we see, the default way of processing does not assign a partitioner; the framework partitions the data in the default way, which is merely a guess. Several ways influence how partitioning takes place, where the easiest is to assign a partitioner."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "A5312336AFC242A5A59FBDBDF31A2102"
    },
    "cell_type" : "code",
    "source" : "val rddpp = rddp.partitionBy(new HashPartitioner(2))",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "810E14041F9D4D848781B1F41C3F3AE9"
    },
    "cell_type" : "code",
    "source" : "rddpp.partitioner",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "DE5CD775ADB648168F9F5E19E503FC60"
    },
    "cell_type" : "markdown",
    "source" : "See how that influences processing; notice that `rddp` is partitioned in the default way, whereas `rddpp` uses only two partitions, based on our instructions."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "18A889521E034581892AEBA30E99F8B9"
    },
    "cell_type" : "code",
    "source" : "val rddpg = rddp.groupByKey()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "5951DAE5B8F44F6F821EC79FF0A81105"
    },
    "cell_type" : "code",
    "source" : "rddpg.toDebugString",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "F59DF38D54FC4BDB847189F24C418DF0"
    },
    "cell_type" : "code",
    "source" : "rddpg.partitioner",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab1005645441-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "0C332CCC142B4326A5826E9BFACEB4A0"
    },
    "cell_type" : "code",
    "source" : "rddpg.partitions.map(p => (p, p.index, p.hashCode))",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "469CF1CFD0264A0487C84246C57197D7"
    },
    "cell_type" : "code",
    "source" : "val rddppg = rddpp.groupByKey()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab1102261949-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "F9D915CA9CF84CC09898F2E0AD618747"
    },
    "cell_type" : "code",
    "source" : "rddppg.partitions.map(p => (p, p.index, p.hashCode))",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "389ED574C2EC44FC9CAD32F0F2E6662C"
    },
    "cell_type" : "markdown",
    "source" : "_Q: Spot the differences between the results, and try to map what you see on Chapter 2 that we read for the course._\n\nObserve how the number of partitions of a groupByKey operation varies depending on the way the input is partitioned. This in turn affects the number of machines that will be at work in subsequent operations. Take a look at the Spark UI to see the difference for the two cases.\n\n_Note: using explicit naming of RDDs helps you keep track of which job corresponds to which case._"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "BB31AD21B3C3432F880826C51F128452"
    },
    "cell_type" : "code",
    "source" : "val pgc2 = rddppg.map( {case(x,y) => (x, y.reduce((a,b) => a + b))} )",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "2633BBDB9BA74879888CC382E37ED878"
    },
    "cell_type" : "code",
    "source" : "pgc2.name = \"Partitioned Group Counts (2)\"",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab1135945678-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "61354A57CA244500ADA1D9BF03A5D33B"
    },
    "cell_type" : "code",
    "source" : "pgc2.take(10)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "31801B9ACEC2461196165FACEA6B187F"
    },
    "cell_type" : "code",
    "source" : "pgc2.partitions.size",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "29B4DE775B534240923B66E5A54BA26F"
    },
    "cell_type" : "code",
    "source" : "pgc2.toDebugString",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "CD42852AFACE4534A74C0AAB6E409A98"
    },
    "cell_type" : "code",
    "source" : "pgc2.partitioner",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "FF85C430DF0640C9AD5A2431FB72960D"
    },
    "cell_type" : "markdown",
    "source" : "Q: do you understand why the partitioner is none? (No worries if not, you will find a clue below.)"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "2C41CAC1ECA447178A6AA4D2609747AA"
    },
    "cell_type" : "code",
    "source" : "val rddp4 = rddp.partitionBy(new HashPartitioner(4))",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab397445376-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "AD1873BFC54444B1A489CA0F728CACD7"
    },
    "cell_type" : "code",
    "source" : "rddp4.take(10)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "BE0EA906F5F845BFB2555A9459CDFF60"
    },
    "cell_type" : "code",
    "source" : "val rddA = rddp4.values.map( x => x  + 10 )",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "E69714ABA270460284C14E82DBBCC829"
    },
    "cell_type" : "code",
    "source" : "rddA.partitioner",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "662AD3585C0F44908B379026555C5A6A"
    },
    "cell_type" : "code",
    "source" : "val rddB = rddp4.mapValues( x => x + 10 )",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "B6AEA92C47C54F89BA307AC32F67BE64"
    },
    "cell_type" : "code",
    "source" : "rddB.partitioner",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "DE8DFAACC314449A857B278F6BBE214F"
    },
    "cell_type" : "markdown",
    "source" : "_Q: Why are the results different for rddA and rddB? How is query processing affected by the partitioners?"
  }, {
    "metadata" : {
      "id" : "0E4FBD8005B445F086EA2BBB43E73A98"
    },
    "cell_type" : "markdown",
    "source" : "Summarizing: partitioning depends on the distributed operations that are executed, and only operations with guarantees about the output distribution will carry an existing partitioner over to its result.\n\nAnother way to control the level of parallellism during query execution is to use the `repartition` and `coalesce` operations. "
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "5010A4D47D804ABA807C14FF64B14A23"
    },
    "cell_type" : "code",
    "source" : "val rddC = rddA.repartition(2)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab1575744403-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "5DA62551D8A04EB189B51D1F0CE666F0"
    },
    "cell_type" : "code",
    "source" : "rddC.partitions.map(p => (p, p.index, p.hashCode))",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "EBD31B1FF4E746A78DE9AE647B5D9CAA"
    },
    "cell_type" : "code",
    "source" : "rddC.toDebugString",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "86BA0B0B70AF47A5807175DC6CDD8E42"
    },
    "cell_type" : "code",
    "source" : "val rddD = rddB.coalesce(2)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "6454D1A814A74E0A9CA9143B5CD24C9D"
    },
    "cell_type" : "markdown",
    "source" : "Remember that we need actions for things to happen - so if you inspect the Spark UI, there are now no query plans corresponding to the above commands. Let us take two samples from the results. Do you understand the jobs and stages that you find in the Spark UI?"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab606463789-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "87D3C20E0AB64045BF1ACCF1A7B14882"
    },
    "cell_type" : "code",
    "source" : "rddC.takeSample(true, 10);",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab1379823251-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "53EBE9CBB13F4802831788A15D983CAF"
    },
    "cell_type" : "code",
    "source" : "rddD.takeSample(true, 10);",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "2CB8AF23FB424943A84E890EF72B5123"
    },
    "cell_type" : "markdown",
    "source" : "_Q: Compare the two query plans for `rddC` and `rddD`. Can you explain why the second query plan has on less shuffle phase?_"
  }, {
    "metadata" : {
      "id" : "BA7896AF195248DC8EE968F80122AB93"
    },
    "cell_type" : "markdown",
    "source" : "## Wrap up\n\nIf you have reached this point propery and understood what you observed, you have a solid understanding of Spark and its execution model.\n\nAssignment 3B will move up the stack to consider the Dataframe API."
  }, {
    "metadata" : {
      "id" : "2C853A8D8A774E3A9D20630DB6E325A4"
    },
    "cell_type" : "markdown",
    "source" : "**See also**\n\n* http://spark.apache.org/examples.html\n* http://spark.apache.org/docs/latest/programming-guide.html\n\nAdditional info on Scala (just in case, some background may be useful to get things done, but do not get carried away - the course is about big data processing, not functional programming!)\n* Main [Scala site](http://scala-lang.org/), [tutorial](http://docs.scala-lang.org/tutorials/scala-for-java-programmers.html) and [API documentation](http://www.scala-lang.org/api/current/index.html)"
  } ],
  "nbformat" : 4
}

{
  "metadata" : {
    "name" : "open-data-Nijmegen",
    "user_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null
  },
  "cells" : [ {
    "metadata" : {
      "id" : "7B36BF513C9645A18A7B800025A5BC05"
    },
    "cell_type" : "markdown",
    "source" : "# Nijmegen Open Data\n\nSpark Notebook to help you analyze data sets provided by Nijmegen's city council."
  }, {
    "metadata" : {
      "id" : "E57A66341EAE4E829036DD82071E0CC7"
    },
    "cell_type" : "markdown",
    "source" : "## Nijmegen data\n\nThe city of Nijmegen provides a variety of resources as open data at\n[nijmegen.nl/opendata](http://www.nijmegen.nl/opendata/).\n\nOur goal is to analyze these together - to investigate if we can say anything about the relation between population statistics in areas of the city and the activities that are organized there.\n\nSpecifically, we will integrate the following two data sets:\n- Streetnames and their quarters: https://open-data.nijmegen.nl/dataset/bag-adressen\n- Public artworks: https://open-data.nijmegen.nl/dataset/kos\n\nYou need to download the data yourself; e.g., in your docker container, execute\n\n    cd /data/bigdata\n    wget http://www.nijmegen.nl/opendata/BAG_ADRES.csv\n    wget -O kunstopstraat-kunstwerk.csv http://www.nijmegen.nl/kos/opendata/\n\nThis notebook contains code to get you started in the analysis, so you can pick up Spark by example and refresh your SQL.\n\nTo fully understand everything, you will need the documentation: http://spark.apache.org/docs/latest/sql-programming-guide.html\n\nHints of interesting things to discuss in the blog for assignment 3 start with  _**Q:**_."
  }, {
    "metadata" : {
      "id" : "5DD9F2B813D4407FBB231EEB39148B39"
    },
    "cell_type" : "markdown",
    "source" : "_Before we start, we add an external java package that we need later to the environment, using a specific spark-notebook directive that needs to go first._"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab619924703-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "23EC796600D94A5DB582BF656756BA86"
    },
    "cell_type" : "code",
    "source" : ":dp\n+ org.orbisgis % cts % 1.4.0",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "430FFDBA0F334048B46A627573B79B21"
    },
    "cell_type" : "markdown",
    "source" : "Next, more preamble, importing some necessary classes and creating a `SparkSession` object."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "B861A5974A85486D8FD774A92C7E0148"
    },
    "cell_type" : "code",
    "source" : "// replacement for old version; used to be \n// val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n\nimport org.apache.spark.sql.types._ \n\nval spark = SparkSession\n   .builder()\n   .appName(\"A3b-spark-df\")\n   .getOrCreate()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "B6D57E06C5744D378257D4211AF8DCB5"
    },
    "cell_type" : "markdown",
    "source" : "### Addresses and Quarters\n\nWe start with the so-called BAG (_basisadministratie adressen en gebouwen_, i.e., records of addresses and buildings), a standardized resource for which the definition is managed at the national level.\n\nLike most data, the BAG is distributed as CSV, Comma-Separated-Values. Just using `split` may work on very small datasets with regular data, but as soon as quoted values contain commas, we would get in trouble. Spark 2.0 comes with support to parse various data formats, including CSV and JSON, so let us use the standard interfaces."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "2C9A3B0D07CE4E478083C575671B9DFE"
    },
    "cell_type" : "code",
    "source" : "//import sparkSession.implicits._\n\nval bagdata = spark.read.format(\"csv\").option(\"header\", true).load(\"/data/bigdata/BAG_ADRES.csv\").cache()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab1009015540-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "AF393ED62F8E4BC7842504982F3F2063"
    },
    "cell_type" : "code",
    "source" : "// Get an impression of the data, easily!\nbagdata.printSchema\nbagdata.show(2)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "EE56D3095D2548618F0819BA50F2E0D6"
    },
    "cell_type" : "markdown",
    "source" : "#### Real-life data\n\nDataFrames are very handy to get a quick impression of the data, e.g., using `show()`, `sample()` and `describe()`."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "0F830ADCA7F54851852433CA6489D0C2"
    },
    "cell_type" : "code",
    "source" : "val addrDF = bagdata.select('STRAAT, 'X_COORD, 'Y_COORD, 'WIJK_OMS)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "FDBEDC68D72F45FD83954FD54D63C8F7"
    },
    "cell_type" : "code",
    "source" : "// Show 5 rows, without truncating values\naddrDF.show(5, false)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "21DB7C87707547178773E81549A2C6E4"
    },
    "cell_type" : "code",
    "source" : "// Describe statistics of the value distribution\naddrDF.describe().show()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "1625B377A87143FC965634CF44A09EC8"
    },
    "cell_type" : "markdown",
    "source" : "If you think a bit longer about the reported statistics, you realize that not all the data is equally well formatted - the data dump contains address records without x and/or y coordinates; which you can validate as follows:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab921940959-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "377BEE57FF13485DAC755D4D1B40EE88"
    },
    "cell_type" : "code",
    "source" : "addrDF.filter( $\"X_COORD\".isNull ).count",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "B7B42B7FEBE14FFFAC17FD96E0FC940C"
    },
    "cell_type" : "markdown",
    "source" : "How to proceed is a decision that the data scientist needs to take - that is, it will be up to your judgement. Can we proceed without these records? In this case, the inconsistent records concern only a very small fraction of the data (45 out of almost 100K records), and we might decide to simply ignore these without valid `(x,y)` coordinates.\n\nIn many real life situations, however, we would need to invest more effort and clean the data to overcome these deficiencies. If you look into examples of records without coordinates (_Q: can you write the small numbers of lines of code to inspect these records?_), you find that some of these records have a value \"Niet authentiek\" in field STATUS, whereas almost all records have \"Naamgeving uitgegeven\". We lack the domain knowledge to understand the details of this labelling, and may need to dig deeper, or even have a chat with the owners of the data to learn how these values are assigned.\n\nKeep in mind that we only discovered this anomaly in the data when we attempted to define a proper schema, and discovered that the data did not satisfy that schema. Switching from transformations over RDDs to the more structured Data Frame representation can help avoid mistakes in the analysis.\n\n**Q:** _Discuss decisions in data selection and data cleaning that you took to complete the assignment in your blog post._"
  }, {
    "metadata" : {
      "id" : "993CFA140F484EDC82E392E71A724C8D"
    },
    "cell_type" : "markdown",
    "source" : "#### Schema\n\nThe Data Frame API supports a more structured interface to data by defining a `case class` to represent the data."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "5DEF2851B34B438DB8F802EDFEA556B6"
    },
    "cell_type" : "code",
    "source" : "case class Addr(street:String, quarter:String, x:Float, y:Float)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "719CCFD68CAC42AD8E04E8FBB273EFCE"
    },
    "cell_type" : "markdown",
    "source" : "Project the desired columns onto the fields of the `case class` and convert to a so-called `dataset`."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "E1B060D1C88545E584677058DCA18580"
    },
    "cell_type" : "code",
    "source" : "val addrDF = bagdata.select($\"STRAAT\" as \"street\",\n                            $\"X_COORD\".cast(FloatType) as \"x\",\n                            $\"Y_COORD\".cast(FloatType) as \"y\",\n                            $\"WIJK_OMS\" as \"quarter\").as[Addr].cache()\naddrDF.show(2)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "DDD4524221FB46A286EEE74E4D080022"
    },
    "cell_type" : "markdown",
    "source" : "Actually, parsing the X and Y coordinates is a little messy: the data contains the X and Y coordinates in the Dutch locale, which means that they are written as 0,5 instead of 0.5.\n\nWe define a function to convert values where possible."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "2EE7A07C08EB49F4BE7218E15289C1C8"
    },
    "cell_type" : "code",
    "source" : "// Use Java's NumberFormat to parse values by their locale \nimport java.text.NumberFormat\nimport java.util.Locale\nval nf = NumberFormat.getInstance(Locale.forLanguageTag(\"nl\")); // Handle floats written as 0,05 instead of 0.05\n\ndef convToFloat(s: String): Option[Float] = {\n  try {\n    Some(nf.parse(s).floatValue)\n  } catch {\n    case e: Exception => None\n  }\n}",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "8E3C1BAD99BD420E9C07FE316437DD78"
    },
    "cell_type" : "markdown",
    "source" : "So what does this actually do? Well, here is an example of using the new `toFloat` class on _some_ data with one clear exception:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab175792067-0\"\n}"
      },
      "id" : "DA5D834BC9224DE6811AAB51EE673892"
    },
    "cell_type" : "code",
    "source" : "Seq( \"0.045\", \"0,054\", \"abc\", \"45.789,34\" ).map( x => convToFloat(x).getOrElse(null))",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "EAD399C79491444D859BD36E69C15900"
    },
    "cell_type" : "markdown",
    "source" : "We have to register this function as a _user-defined function_ so it can be used in Spark SQL."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "1ACD24DF278842079AFCFE338AEDF9CC"
    },
    "cell_type" : "code",
    "source" : "// Registering a user-defined function that handles null values as well\nval tfloat = udf((f: String) => convToFloat(f).getOrElse(0f))\n\n// Apply the transformation to the x and y columns\nval addrDF = bagdata.select($\"STRAAT\" as \"street\",\n                            tfloat($\"X_COORD\").cast(FloatType) as \"x\",\n                            tfloat($\"Y_COORD\").cast(FloatType) as \"y\",\n                            $\"WIJK_OMS\" as \"quarter\").as[Addr].cache()\naddrDF.show(2)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "B55CE28CAF3A4B63B80E3618A45BFF8C"
    },
    "cell_type" : "markdown",
    "source" : "Let's check that the missing values have been placed on the origin:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "00E7170B80CA4EA9BC3EC8B7CB541181"
    },
    "cell_type" : "code",
    "source" : "printf(\n  \"%d points at origin, %d null values\",\n  addrDF.filter(\"x = 0 or y = 0\").count,\n  addrDF.filter('x.isNull or 'y.isNull).count\n)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "039D4E2B27874DBAA0A20ABF90698C14"
    },
    "cell_type" : "markdown",
    "source" : "Use `describe` to get an overview of the data:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "905EE2F864204B408A28BEF5046EC495"
    },
    "cell_type" : "code",
    "source" : "addrDF.describe().show() // Note: describe() returns a dataframe, to which we apply the show() command",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "8D6A7F6D606A460AAF08C132FDB7FF3E"
    },
    "cell_type" : "markdown",
    "source" : "#### Using Data Frame Operators"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "524E39515785443EBFBF0BA269C26D22"
    },
    "cell_type" : "code",
    "source" : "val qc_1 = addrDF.groupBy(\"quarter\").count.cache()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "FE42732588C94DEF935D76D130F2F6F9"
    },
    "cell_type" : "code",
    "source" : "qc_1.show(5)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "FC80E0AFBF5B4F718ADB0483155C2F1A"
    },
    "cell_type" : "markdown",
    "source" : "What are the top 10 largest quarters of Nijmegen?"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "22671278EFEC4D57B75A079EBE7D2F10"
    },
    "cell_type" : "code",
    "source" : "val qc_1_top = qc_1.orderBy(desc(\"count\")).limit(10)\nqc_1_top.show()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "0AAF751C56294FFBAFD6A7026AB8EB9E"
    },
    "cell_type" : "markdown",
    "source" : "Wondering what happens inside? The `explain` operator gives you the _physical_ query plan. If you add a boolean, you also see the logical plans, both the initial as parsed, and the plan that is the result of query optimization.\n\n_It is okay if you do not fully understand the full query plan - but do try for a minute or so._"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "C849377B8FDF456986830FC46DC86BCA"
    },
    "cell_type" : "code",
    "source" : "println(\"Group by and count:\")\nprintln(\"===================\")\nqc_1.explain(true)\n\nprintln(\"Order descending limit 10:\")\nprintln(\"==========================\")\nqc_1_top.explain(true)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "46A33671A60C46BE83C34AECC48A1890"
    },
    "cell_type" : "markdown",
    "source" : "#### Using SQL"
  }, {
    "metadata" : {
      "id" : "C3A6C9731A5E4F5B8FC83393F9FAACA1"
    },
    "cell_type" : "markdown",
    "source" : "You are not restricted to building queryplans yourself by applying operators to Data Frames; instead, you can also use the SQL interface, and, mix and match SQL querying with follow-up operations using the Data Frame API, or even convert the data back to RDDs and continue to perform analyses directly working with RDDs.\n\nUsing SQL is most convenient when queries get larger and more complicated. Consider for example the query for the 10 largest quarters:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "70FD626286454C2684CCB33C63DC103B"
    },
    "cell_type" : "code",
    "source" : "addrDF.createOrReplaceTempView(\"addresses\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "47B6549888214275A5D50A6E50768194"
    },
    "cell_type" : "code",
    "source" : "val qc_2_top = spark.sql(\"SELECT quarter, count(quarter) AS qc FROM addresses GROUP BY quarter ORDER BY qc DESC LIMIT 10\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "16FE87C02753417DAE5BD57FB16E0481"
    },
    "cell_type" : "code",
    "source" : "qc_2_top.show",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "261781F463F046D2986EA105B8C87FD0"
    },
    "cell_type" : "code",
    "source" : "qc_2_top.explain(true)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "B7E07A735BFB47C6881CE5818ED0478C"
    },
    "cell_type" : "markdown",
    "source" : "### Artworks\n\nNow that we have the address data prepared, move on to look into the data about the artworks.\n\nLoad the data, look at the schema and global statistics, and inspect a small sample:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "E3EAA5EDD64041409F95BFFE263FE637"
    },
    "cell_type" : "code",
    "source" : "val kunst = spark.read\n    .format(\"csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .load(\"/data/bigdata/kunstopstraat-kunstwerk.csv\").cache()\n",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "1728A743089D442E8F9F9EDE7BE99747"
    },
    "cell_type" : "code",
    "source" : "kunst.printSchema",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "0838E3F28FBB49089455CF2AFCECAC53"
    },
    "cell_type" : "code",
    "source" : "// Select all the public art created before the year 2000\nval kunstwerken = kunst.select(\"naam\",\"locatie\",\"latitude\",\"longitude\",\"bouwjaar\",\"url\").where(\"bouwjaar <= 1999\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "D429016909A9411989AB6CD476D9701A"
    },
    "cell_type" : "code",
    "source" : "kunstwerken.sample(true,0.1).show()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "0B024D5FBECD4BC9A12E8BBA8C1231F4"
    },
    "cell_type" : "markdown",
    "source" : "The coordinates are not correctly detected as floats - again, due to the Locale.\nA quick hack (but not robust code) to cast the values to floats:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "86ACB3FFDD5241F3B4B71E668528157F"
    },
    "cell_type" : "code",
    "source" : "// UGLY: this string transform needs to be replaced by proper Locale handling...\nval kunstxy = kunstwerken\n  .withColumn(\"latitude\", translate(kunstwerken.col(\"latitude\"),\",\",\".\").cast(\"float\"))\n  .withColumn(\"longitude\", translate(kunstwerken.col(\"longitude\"),\",\",\".\").cast(\"float\"))",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "BA382E45FC8441CC85069152862E47C4"
    },
    "cell_type" : "markdown",
    "source" : "Let's inspect some of the data, using global information and a sample."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "88809D92DB1048908164A26F8EA30623"
    },
    "cell_type" : "code",
    "source" : "kunstxy.sample(true,0.05).show()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "B3B112064FF74E238831D15A7C25EB51"
    },
    "cell_type" : "markdown",
    "source" : "Artworks created during WWII:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "4F84F3F8578F4B2183942FA51713039D"
    },
    "cell_type" : "code",
    "source" : "kunstxy.filter(\"bouwjaar >= 1940 and bouwjaar <= 1945\").show()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "313A89394977426AB908BD2122F43C0D"
    },
    "cell_type" : "code",
    "source" : "kunstxy.describe().show()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "55AD2AB4EE7B4A8183A276DE8F9CC307"
    },
    "cell_type" : "markdown",
    "source" : "One item has no location information, given the difference in counts between `naam` and `latitude`.\n\nLet's inspect the problematic tuple (using SQL - _\"niet omdat het moet, maar omdat het kan\"_):"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "524FDAFD474A4E6D81D8461903928D28"
    },
    "cell_type" : "code",
    "source" : "kunstxy.createOrReplaceTempView(\"kunstxy\")\nspark.sql(\"SELECT * FROM kunstxy WHERE locatie IS NULL\").show()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "F7DF75AADB1646A68EC682FA6BA8F95E"
    },
    "cell_type" : "markdown",
    "source" : "If you would use the data in practice, you can simply ignore this one, or define a new table that excludes the NULL values.\n\nLet us however go back to the source data, in the Dataframe called `kunst`, and look into the dataset in more detail:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "F981DF79F153442E918E4F11B3D10BE4"
    },
    "cell_type" : "code",
    "source" : "kunst.createOrReplaceTempView(\"kunst\")\nkunst.describe().show()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "F7DF75AADB1646A68EC682FA6BA8F95E"
    },
    "cell_type" : "markdown",
    "source" : "Even the counts of tuples with longitude values does not equal the counts with latitude values... and a mean \"bouwjaar\" in the future is, well, suspicious!\n\nLet's investigate in more detail, listing missing lat/lon and out-of-range year values in a few SQL queries.\n\n__Q:__ _Try to understand the various data problems we encounter when working on this dataset. Which are fundamental, and which are just an artifact of our processing?_"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "F3983427B81B4EE580A273DB8EF6E06E"
    },
    "cell_type" : "code",
    "source" : "spark.sql(\"select * from kunst where (latitude is not null and longitude is null) or (latitude is null and longitude is not null)\").show(15)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "1E474D5F8504419E800C773FF3E25F5C"
    },
    "cell_type" : "code",
    "source" : "spark.sql(\"select * from kunst where bouwjaar > 2017\").show(10)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "8B23DD283EB8471FA0618B0F78FBC13C"
    },
    "cell_type" : "code",
    "source" : "spark.sql(\"select * from kunst where bouwjaar is null\").show(10)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "A40AC6D417D64F72846896702165C891"
    },
    "cell_type" : "markdown",
    "source" : "### Art on the map\n\nTo continue, let us first create an artworks dataset that is relatively clean, removing tuples that resulted from parsing errors and/or those with missing values.\n\nCan we then find out which quarters have the most artworks?\nPerhaps we can even identify the development of the city over time using the dates of these artworks?\n\nTo answer these questions, we need to join the dataset with addresses with the one with the artworks.\nThe best link between the two datasets seems to be the coordinates. Let's see how to exploit the location fields to answer our questions."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "AAE28B97FAFF459D8C6FB1550ACEF92E"
    },
    "cell_type" : "code",
    "source" : "val ks = spark.sql(\"select * from kunst where (latitude is not null and longitude is not null) and bouwjaar < 9999\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "FE98462F08C144C19ACA412DDC66FDEA"
    },
    "cell_type" : "code",
    "source" : "ks.describe()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "C4D83E573C854C598A6A6A9C9517A963"
    },
    "cell_type" : "code",
    "source" : "val kos = ks\n  .withColumn(\"latitude\", translate(ks.col(\"latitude\"),\",\",\".\").cast(\"float\"))\n  .withColumn(\"longitude\", translate(ks.col(\"longitude\"),\",\",\".\").cast(\"float\"))\n  .withColumn(\"bouwjaar\",col(\"bouwjaar\").cast(\"int\"))\n  .select(\"naam\",\"locatie\",\"latitude\",\"longitude\",\"bouwjaar\",\"url\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "C68DC0321A2744768B966F984ACDEDF4"
    },
    "cell_type" : "code",
    "source" : "kos.createOrReplaceTempView(\"kos\")\nkos.printSchema",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab315996847-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "E951D6BF4BA8485EA1238AD460593F07"
    },
    "cell_type" : "code",
    "source" : "kos.show(15, false)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "4091C7DF738B47328BDB2C70B19773EC"
    },
    "cell_type" : "code",
    "source" : "// Metadata from the catalogue\nspark.catalog.listDatabases.show(false)\nspark.catalog.listTables.show(false)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "66DF06B3725C44BF88226486AD80CF96"
    },
    "cell_type" : "markdown",
    "source" : "#### Coordinates, coordinates\n\nBoth datasets include (x,y) locations, but they are in different coordinate systems. BAG uses an official Dutch system known as RD New, from \"Rijksdriehoeksco??rdinaten\" (~ \"national triangle coordinates\").\n\nThe other dataset uses (lat,lon) coordinates to show artworks on the map, see e.g.:\n[\"de pleinenroute\"](http://www.nijmegen.nl/kos/kunstroute.aspx?id=1) (Route of the squares) \n\nLuckily, we are not the first who need to convert values between coordinate systems - I found the following \"easy-to-use\" (compared to geo-informatics alternatives) \nJava library:\n[Coordinate Transformation Suite (CTS)](https://github.com/orbisgis/cts). \n\n_(We already loaded this library using the `:dp` directive at the top of the notebook.)_\n\nThe following snippet of Java/Scala code sets up the library to transform map coordinates to RD. To use an external library in Spark, all the objects must be serializable; as they are shipped to the worker nodes. Here, this is achieved by using the `@transient` directive to instruct Scala not to include this part of the\nobject in the serialization, in combination with checking for `null` values when using these variables.\n\nMore information, if you want to dig deeper:\n* RD New, or \"Amersfoort\": [Wikipedia entry](https://nl.wikipedia.org/wiki/Rijksdriehoeksco%C3%B6rdinaten)\n* The transformation: http://pdok-ngr.readthedocs.io/handleidingen.html#coord-trans\n* Serialization and `object` vs. `class`: http://spark.apache.org/docs/latest/programming-guide.html#passing-functions-to-spark\n* Role of \"annotation\" `@transient`: http://fdahms.com/2015/10/14/scala-and-the-transient-lazy-val-pattern/\n\n_Understanding all the details of the `CT` class and its inner workings is not necessary to complete assignment 3._"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "AF215715756C461283481E1450C05C96"
    },
    "cell_type" : "code",
    "source" : "object CT extends Serializable {\n  \n  import org.cts.CRSFactory;\n  import org.cts.crs.GeodeticCRS;\n  import org.cts.registry.EPSGRegistry;\n  import org.cts.op.CoordinateOperationFactory;\n  import org.cts.op.CoordinateOperation;\n\n  // global variables to keep state for transformations\n  @transient private var xy2latlonOp : CoordinateOperation = null;\n  @transient private var latlon2xyOp : CoordinateOperation = null;\n\n  // Create the coordinate transformation functions to convert from RD New to WGS:84 and vice versa\n  def initTransforms : (CoordinateOperation, CoordinateOperation) = {\n    // Create a new CRSFactory, a necessary element to create a CRS without defining one by one all its components\n    val cRSFactory = new CRSFactory();\n\n    // Add the appropriate registry to the CRSFactory's registry manager. Here the EPSG registry is used.\n    val registryManager = cRSFactory.getRegistryManager();\n    registryManager.addRegistry(new EPSGRegistry());\n\n    // CTS will read the EPSG registry seeking the 4326 code, when it finds it,\n    // it will create a CoordinateReferenceSystem using the parameters found in the registry.\n    val crs1 : GeodeticCRS = (cRSFactory.getCRS(\"EPSG:28992\")).asInstanceOf[GeodeticCRS];\n    val crs2 : GeodeticCRS = (cRSFactory.getCRS(\"EPSG:4326\") ).asInstanceOf[GeodeticCRS];\n    \n    // Transformation (x,y) -> (lon,lat)\n    val xy2latlonOps = CoordinateOperationFactory.createCoordinateOperations(crs1,crs2);\n    val xy2latlon = xy2latlonOps.get(0);\n    \n    val latlon2xyOps = CoordinateOperationFactory.createCoordinateOperations(crs2,crs1);\n    val latlon2xy = latlon2xyOps.get(0);\n    \n    (xy2latlon, latlon2xy)\n  }\n\n  // Encapsulate private transient variable (for serializability of the object)\n  def getXYOp : CoordinateOperation = {\n    if (xy2latlonOp == null){\n      val ts = initTransforms\n      xy2latlonOp = ts._1\n      latlon2xyOp = ts._2\n    }\n    xy2latlonOp\n  }\n\n  // Encapsulate private transient variable (for serializability of the object)\n  def getLatLonOp : CoordinateOperation = {\n    if (latlon2xyOp == null){\n      val ts = initTransforms\n      xy2latlonOp = ts._1\n      latlon2xyOp = ts._2\n    }\n    latlon2xyOp\n  }\n  \n  // Use the library's transformation function to convert the coordinates\n  def transformXY(x:Float, y:Float) : (Float, Float) = {   \n    // Note: easily confused, (lat,lon) <-> (y,x)\n    val lonlat = this.getXYOp.transform(Array(x.toDouble, y.toDouble));\n    return ( lonlat(1).toFloat, lonlat(0).toFloat)\n  }\n  \n  // Use the library's transformation function to convert the coordinates\n  def transformLatLon(lat:Float, lon:Float) : (Float, Float) = {\n    // Note: easily confused, (lat,lon) <-> (y,x)\n    val xy = this.getLatLonOp.transform(Array(lon.toDouble, lat.toDouble));\n    return ( xy(0).toFloat, xy(1).toFloat)\n  }\n}",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "564013FEB2AD495D848C0DCFD549E8D9"
    },
    "cell_type" : "markdown",
    "source" : "Using the transformation from RD New to WGS:84 (the latitude/longitude pairs used in google maps and open streetmap), we can now plot our BAG data on a map.\n\nConsider the following example, where we take a sample of points from the address data that corresponds to _Toernooiveld_ and put these on the map of Nijmegen, using the widgets provided by spark notebook."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "3F13AB4552274DC8BE91F5ACD7D573DD"
    },
    "cell_type" : "code",
    "source" : "// Register the transformation function as UDF and apply it to the BAG dataframe\nval txyudf = udf( CT.transformXY _ )\nval ta = addrDF.withColumn(\"latlon\", txyudf($\"x\",$\"y\"))",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "A59D313E5BD14F0181533007B68AA7E7"
    },
    "cell_type" : "code",
    "source" : "val ds = ta.filter('street === \"Toernooiveld\").select('street, 'x, 'y, 'latlon .getField(\"_1\") as \"lat\", 'latlon .getField(\"_2\") as \"lon\")\n//ds.show",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "0C6EFF8E9FA54B209B844D37DAC2E540"
    },
    "cell_type" : "code",
    "source" : "val w = GeoPointsChart(ds.select('lat, 'lon))\nw",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "7BDABEEEB1E74C86822BB30DA3AAB55C"
    },
    "cell_type" : "code",
    "source" : "// Cell left empty on purpose, to encourage you to plot your own street data on the map!\n\n",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "4C09F8644FF94B7284F9A4F9933AABFF"
    },
    "cell_type" : "markdown",
    "source" : "After all this fun with the open address data, we'd almost forget that we set out to join the two datasets!\n\nLet's switch back to SQL, and use the artworks as a starting point.\n\n__Q:__ _Why would you apply the transformation on the artworks dataset to join the result against the addresses, instead of vice versa?_"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "CDF77718E8AE4972887C5347498C6EB8"
    },
    "cell_type" : "code",
    "source" : "spark.udf.register(\"transformLatLon\", (lat:Float, lon:Float) => CT.transformLatLon(lat,lon))",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "99CB57D905C84BA8A6CFCEBB9270EE63"
    },
    "cell_type" : "code",
    "source" : "val kosxy = spark.sql(\"select naam, bouwjaar, transformLatLon(latitude, longitude) as XY from kos\")\nkosxy.createOrReplaceTempView(\"kosxy\")",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "96C8F31EF9EE4E4180DFEA093B884406"
    },
    "cell_type" : "code",
    "source" : "val kosquarter = spark.sql(\n  \"select distinct naam, quarter, min(bouwjaar) as jaar from kosxy, addresses \" +\n  \"where abs(XY._1 - x) < 10.0 and abs(XY._2 - y) < 10.0 \" +\n  \"group by naam, quarter \"\n).cache()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "661FE56E507A449B8B96ABB7409A3A6B"
    },
    "cell_type" : "code",
    "source" : "kosquarter.show(20,false)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "9F65A5A0B684415A9120B86E1351A2B7"
    },
    "cell_type" : "markdown",
    "source" : "Let us now answer the question we started out with initially: can we infer the growth of the city of Nijmegen through the years from the years in which the artworks were created?"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "9A072D55670B4F7B837E070576019B22"
    },
    "cell_type" : "code",
    "source" : "// A nice example where SQL is a good choice to answer the information need:\nkosquarter.createOrReplaceTempView(\"kosquarter\")\nspark.sql(\"select distinct quarter, min(jaar) as jaar from kosquarter group by quarter order by jaar\").show(100,false)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "BF7C78C3BAC4431F8AA0DB7ED2ACA579"
    },
    "cell_type" : "markdown",
    "source" : "A few missing city quarters in this list. Do they really have no artworks, or did we not manage to find their corresponding quarter using the spatial join on coordinates?\n\nInspecting the artworks not matched up with their quarters is easy, and there are quite a few of those:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "91D446FF523F47E787F871359CD50DD8"
    },
    "cell_type" : "code",
    "source" : "// Artworks not found in the spatial join result:\nspark.sql(\"select naam from kos where naam not in (select naam from kosquarter)\").count",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "6F12D2BE815E4AA08E72F9268ECBC7AC"
    },
    "cell_type" : "markdown",
    "source" : "__Q:__ _Can you produce the list of quarters that is missing because no artwork has been situated in the quarter?_\n\n__Q:__ _Can you produce a longer list of quarters and their oldest artworks?_\n\n__Q:__ _What are the years associated to artworks not yet matched up with the addresses database? What does this mean for our initial research question?_"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "395F3566DFF946C783487B8A39C34CEA"
    },
    "cell_type" : "code",
    "source" : "// Cell empty on purpose, try your approach here!\n\n",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "D6F075489444489D9A0A5538AD5B2012"
    },
    "cell_type" : "markdown",
    "source" : "Done too quickly? Haven't had enough of open data yet?\n\nExtend your data analysis of the city with more data, digging into the CBS data for Nijmegen.\n\nA variety of statistics about the population has been made available in this dataset: https://open-data.nijmegen.nl/dataset/statistische-data\n\n    wget http://www.nijmegen.nl/opendata/opendata_stadsgetallen.accdb\n    \nHow to convert this Microsoft Access data to CVS: https://rubigdata.github.io/course/background/access2csv.html"
  } ],
  "nbformat" : 4
}
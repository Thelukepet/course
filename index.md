---
layout: page
title: Course Information
tagline: RU Big Data NWI-IBC036-2015
description: RU Big Data course information and overall structure
---

## Related Course Information

The course on Blackboard:
[Blackboard](http://bit.ly/RUBigDataBB)

The course on Piazza:
[Piazza](http://bit.ly/RUBigData)

Quizzes:
[Socrative](https://b.socrative.com/login/student/), room BIGDATARU

## Practical Assignments

### Objectives

The practical work for the course serves two main objectives:
1. Get to know the emerging Big Data platforms in more detail by using them, with a focus on data analysis using Spark.
2. Gain hands-on experience using modern development tools and services, including `git`, `github`, `docker`, `Jupyter notebooks`, *etc.*.

### Piazza

Cluster computing is a dynamic field characterized by continuous developments.
Many tools are not as robust as they should be!
We should expect many surprises and puzzling issues to arise during practical work for the course.
Please help each other!

[Sign up](https://piazza.com/ru.nl/spring2016/nwiibc036) to Piazza and help each other during practical work.
Feel free to ask for help, with any type of problem - whether it being a deep technical issue, or just the question "how did I again set my ssh key",
using your own name and account, or anonymously if you prefer.
The forum works best if we use it actively!

### Completing and Evaluating Assignments

In this course, we use *Github for education* to hand in **brief** reports (like a blog post) about the results of the 
practical work. Standard git functionality serves perfectly for carrying out peer review, and keeping track of 
modifications in response to feedback.

Refer to [assignment 1a](assignments/A1a-blogging.html) for more details about the blogging framework.

### Development environment

To reduce the source of errors in setting up compute infrastructure, I decided to try Docker for the course.

Refer to [assignment 1b](assignments/A1b-docker.html) to get to know Docker and install Spark Notebook,
that we will use this week's practical assignment for learning Spark.

### Assignments Schedule

#### February 26th: Setup

Completing the first assignment is not evaluated. However, I expect that every student will be able 
to create their assignment reports in Markdown and have these converted to HTML at their github page,
and know how to start Spark Notebook on their hardware of choice (be it a laptop, their home computer or
a PC in HG 00.023).

Instructions (assignments A1a and A1b):

* [Assignment 1a](assignments/A1a-blogging.html)
* [Assignment 1b](assignments/A1b-docker.html)

#### March 4th: Spark 101

Start work on assignment 2A, to get a basic understanding of working with 
the Spark Notebook.

* [Assignment 2a](assignments/A2a-spark-101.html)

#### Wednesday March 9th:
Start work on assignment 2B, solving various counting problems over text.

* [Assignment 2b](assignments/A2b-execution-model.html)

#### ~~Tuesday March 15th:~~ _Thursday March 17th_
Deadline for handing in your blog on assignment 2 (parts A and B).
**Any time of the day is fine.**

#### ~~~Wednesday March 16th:~~~
Start peer review assignment 2 (detailed instructions will follow).
Start on assignment 3, solving a more advanced data science problem.

#### Tuesday April 5th:
Test A about all lectures and reading material Q3.

#### ~~~Tuesday March 22nd:~~~
End of peer review period for assignment 2.

#### ~~~Tuesday March 29th:~~~
Deadline final version of blog for assignment 2, 
updated based on input peer review.
Deadline for handing in your blog on assignment 3.

#### ~~~Wednesday March 30th:~~~
Start peer review assignment 3.

### TBA
~~~Deadline peer review assigment 3.~~~

#### ~~~Tuesday April 12th:~~~
Deadline final version of blog for assignment 3.

Note:
Students who only follow Webscale AI have then completed the Big Data 
component of their course.

The Big Data students and the Webscale AI students who take the special 
3 ECTS elective continue with the course, which includes one prepatory
assignment 4 and a final project assignment 5.
This final assignment will eventually entail running analysis jobs over
the full CommonCrawl webcrawl, using Hathi, the national Hadoop cluster 
maintained by SURFsara. Access to Hathi requires successful completion of
assignments 2 - 4.

